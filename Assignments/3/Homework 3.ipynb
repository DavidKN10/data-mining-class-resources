{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33569fcefe0bbba2",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf41bd7108f5dc",
   "metadata": {},
   "source": [
    "## Recitation Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51642fd2bf3dd92f",
   "metadata": {},
   "source": [
    "### 1.1 Chapter 13\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 1: Given the following points: 2, 4, 10, 12, 3, 20, 30, 11, 25. Assume $k=3$ and that we randomly pick the initial means $\\mu_1=2$, $\\mu_2=4$, and $\\mu_3=6$. Show the clusters obtained using K-means algorithm after one iteration, and show the new means for the next iteration. \n",
    "\n",
    "<br>\n",
    "\n",
    "Initial cluster:\n",
    "<br>\n",
    "$C_1 = {2, 3}$\n",
    "<br>\n",
    "$C_2 = {4}$\n",
    "<br>\n",
    "$C_3 = {10, 11, 12, , 20, 25, 30}$\n",
    "\n",
    "<br>\n",
    "\n",
    "Mean of the new clusters:\n",
    "<br>\n",
    "$\\mu_1 = \\frac{2+3}{2} = 2.5$\n",
    "<br>\n",
    "$\\mu_2 = \\frac{4}{1} = 4$\n",
    "<br>\n",
    "$\\mu_3 = \\frac{10+11+12+20+25+30}{6} = \\frac{108}{6} = 18$\n",
    "\n",
    "<br>\n",
    "\n",
    "The new means for the next iteration are: $\\mu_1 = 2.5$, $\\mu_2 = 4$, and $\\mu_3 = 18$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 3: Given the two-dimensional points in Table 13.2, assume that $k=2$, and that initially the points are assigned to clusters as follows: $C_1={x_1, x_2, x_4}$ and $C_2={x_3, x_5}$. Answer the following questions:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& X_1 & X_2\\\\\n",
    "\\hline\n",
    "x^{T}_{1} & 0 & 2\\\\\n",
    "x^{T}_{2} & 0 & 0\\\\\n",
    "x^{T}_{3} & 1.5 & 0\\\\\n",
    "x^{T}_{4} & 5 & 0\\\\\n",
    "x^{T}_{5} & 5 & 2 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### a.) Apply the K-means algorithm until convergence, that is, the clusters do not change, assuming (1) the usual Euclidian distance or the $L_2$-norm as the distance between points, defined as $||x_i - x_j||_{2}=(\\sum^{d}_{a=1} (x_{ia} - x_{ja})^2)^{\\frac{1}{2}}$, and (2) the Manhattan distance or the $L_1$-norm defined as $||x_i - x_j||_{1}=\\sum^{d}_{a=1} |x_{ia} - x_{ja}|$.\n",
    "\n",
    "<br>\n",
    "\n",
    "The given data points are: $x_1(0,2)$, $x_2(0,0)$, $x_3(1.5,0)$, $x_4(5,0)$, $x_5(5,2)$ and the two clusters are $C_1(x_1, x_2, x_4)$ and $C_2(x_3, x_5)$. \n",
    "<br>\n",
    "Initialization of the means: $\\mu_1=(1.66, 0.66)$ and $\\mu_2=(3.25, 1)$\n",
    "\n",
    "<br>\n",
    "\n",
    "Suppose $D_{ij}$ represents the distance between the mean of the ith and jth data point. Using the Euclidian distance, the distance between points and means of the clusters calculated are as below:\n",
    "<br>\n",
    "Distance of $x_1$ from the mean of clusters is: $D_{11}=2.13$, $D_{21}=3.40$\n",
    "<br>\n",
    "Distance of $x_2$ from the mean of clusters is: $D_{12}=1.78$, $D_{22}=3.40$\n",
    "<br>\n",
    "Distance of $x_3$ from the mean of clusters is: $D_{13}=3.40$, $D_{23}=2.01$\n",
    "<br>\n",
    "Distance of $x_4$ from the mean of clusters is: $D_{14}=0.67$, $D_{24}=2.01$\n",
    "<br>\n",
    "Distance of $x_5$ from the mean of clusters is: $D_{15}=3.59$, $D_{25}=2.01$\n",
    "<br>\n",
    "\n",
    "Looking at the above distances, we can see that points $x_1$, $x_2$, and $x_4$ have less distance to cluster 1 and data points $x_3$ and $x_5$ have less distance to cluster 2. There is no change in the clusters therefore convergence is achieved. \n",
    "\n",
    "<br> \n",
    "\n",
    "Below are the distance between points and means of the clusters using the Manhattan distance:\n",
    "<br>\n",
    "Distance of $x_1$ from the mean of clusters is: $D_{11}=3$, $D_{21}=4.25$\n",
    "<br>\n",
    "Distance of $x_2$ from the mean of clusters is: $D_{12}=2.32$, $D_{22}=4.25$\n",
    "<br>\n",
    "Distance of $x_3$ from the mean of clusters is: $D_{13}=4$, $D_{23}=2.75$\n",
    "<br>\n",
    "Distance of $x_4$ from the mean of clusters is: $D_{14}=0.82$, $D_{24}=2.75$\n",
    "<br>\n",
    "Distance of $x_5$ from the mean of clusters is: $D_{15}=4.68$, $D_{25}=2.75$\n",
    "<br>\n",
    "\n",
    "Looking at the above distances, we can see that points $x_1$, $x_2$, and $x_4$ have less distance to cluster 1 and data points $x_3$ and $x_5$ have less distance to cluster 2. There is no change in the clusters therefore convergence is achieved. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### b.) Apply the EM algorithm with $k=2$ assuming that the dimensions are independent. Show one complete execution of the expectation and maximization steps. Start with the assumption that $P(C_i | x_{ja})=0.5$ for $a=1,2$ and $j=1,...,5$.\n",
    "\n",
    "<br>\n",
    "\n",
    "The given data points are: $x_1(0,2)$, $x_2(0,0)$, $x_3(1.5,0)$, $x_4(5,0)$, $x_5(5,2)$ and the two clusters are $C_1(x_1, x_2, x_4)$ and $C_2(x_3, x_5)$. \n",
    "<br>\n",
    "Initialization of the mean, covariance matrix, and probability:\n",
    "<br> \n",
    "$\\mu_1=(1.66, 0.66)$ and $\\mu_2=(3.25, 1)$\n",
    "<br>\n",
    "$\n",
    "Cov_1 = \n",
    "\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$ \n",
    "and \n",
    "$Cov_2 = \n",
    "\\begin{pmatrix}\n",
    "1 & 0\\\\\n",
    "0 & 1\n",
    "\\end{pmatrix}\n",
    "$\n",
    "<br>\n",
    "$P(C_1)=0.5$ and $P(C_2)=0.5$\n",
    "\n",
    "<br>\n",
    "\n",
    "In the expectation step, the posterior probability of each point for each cluster is calculated. The probability of $x_j$ belonging to the cluster $C_i$ is given:\n",
    "$$P(C_i|x_j)=\\frac{f_i (x_j | \\mu_i, \\sigma_i^2) P(C_i)}{\\sum^{k}_{a=1} f(x_j | \\mu_a, \\sigma_a^2) \\cdot P(C_a)}$$ \n",
    "where $f(x|\\mu_i, \\sigma^2_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma_i}e^{-\\frac{(x-\\mu_i)^2}{2\\sigma^2_i}}$\n",
    "<br>\n",
    "\n",
    "The univariate normal $x_1$ and both clusters: \n",
    "<br>\n",
    "$f(x_1 |\\mu_1, \\sigma^2_1)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_1 - \\mu_1||}{2}}=0.13$\n",
    "<br>\n",
    "$f(x_1 |\\mu_2, \\sigma^2_2)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_1 - \\mu_2||}{2}}=0.07$\n",
    "\n",
    "<br>\n",
    "\n",
    "The univariate normal $x_2$ and both clusters: \n",
    "<br>\n",
    "$f(x_2 |\\mu_1, \\sigma^2_1)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_2 - \\mu_1||}{2}}=0.16$\n",
    "<br>\n",
    "$f(x_2 |\\mu_2, \\sigma^2_2)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_2 - \\mu_2||}{2}}=0.07$\n",
    "\n",
    "<br>\n",
    "\n",
    "The univariate normal $x_3$ and both clusters: \n",
    "<br>\n",
    "$f(x_3 |\\mu_1, \\sigma^2_1)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_3 - \\mu_1||}{2}}=0.07$\n",
    "<br>\n",
    "$f(x_3 |\\mu_2, \\sigma^2_2)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_3 - \\mu_2||}{2}}=0.14$\n",
    "\n",
    "<br>\n",
    "\n",
    "The univariate normal $x_4$ and both clusters: \n",
    "<br>\n",
    "$f(x_4 |\\mu_1, \\sigma^2_1)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_4 - \\mu_1||}{2}}=0.28$\n",
    "<br>\n",
    "$f(x_4 |\\mu_2, \\sigma^2_2)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_4 - \\mu_2||}{2}}=0.14$\n",
    "\n",
    "<br>\n",
    "\n",
    "The univariate normal $x_5$ and both clusters:\n",
    "<br> \n",
    "$f(x_5 |\\mu_1, \\sigma^2_1)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_5 - \\mu_1||}{2}}=0.06$\n",
    "<br>\n",
    "$f(x_5 |\\mu_2, \\sigma^2_2)=\\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{||x_5 - \\mu_2||}{2}}=0.14$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability of $x_1$ belonging to both can be calculated as: \n",
    "<br>\n",
    "$P(C_1 | x_1)=\\frac{0.13 \\times 0.5}{0.13 \\times 0.5 + 0.16 \\times 0.5 + 0.07 \\times 0.5 + 0.28 \\times 0.5 + 0.06 \\times 0.5}=0.18$\n",
    "<br>\n",
    "$P(C_2 | x_1)=\\frac{0.07 \\times 0.5}{0.07 \\times 0.5 + 0.07 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5}=0.12$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability of $x_2$ belonging to both can be calculated as: \n",
    "<br>\n",
    "$P(C_1 | x_2)=\\frac{0.16 \\times 0.5}{0.13 \\times 0.5 + 0.16 \\times 0.5 + 0.07 \\times 0.5 + 0.28 \\times 0.5 + 0.06 \\times 0.5}=0.22$\n",
    "<br>\n",
    "$P(C_2 | x_2)=\\frac{0.07 \\times 0.5}{0.07 \\times 0.5 + 0.07 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5}=0.12$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability of $x_3$ belonging to both can be calculated as: \n",
    "<br>\n",
    "$P(C_1 | x_3)=\\frac{0.07 \\times 0.5}{0.13 \\times 0.5 + 0.16 \\times 0.5 + 0.07 \\times 0.5 + 0.28 \\times 0.5 + 0.06 \\times 0.5}=0.1$\n",
    "<br>\n",
    "$P(C_2 | x_3)=\\frac{0.14 \\times 0.5}{0.07 \\times 0.5 + 0.07 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5}=0.25$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability of $x_4$ belonging to both can be calculated as: \n",
    "<br>\n",
    "$P(C_1 | x_4)=\\frac{0.28 \\times 0.5}{0.13 \\times 0.5 + 0.16 \\times 0.5 + 0.07 \\times 0.5 + 0.28 \\times 0.5 + 0.06 \\times 0.5}=0.4$\n",
    "<br>\n",
    "$P(C_2 | x_4)=\\frac{0.14 \\times 0.5}{0.07 \\times 0.5 + 0.07 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5}=0.25$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability of $x_5$ belonging to both can be calculated as: \n",
    "<br>\n",
    "$P(C_1 | x_5)=\\frac{0.06 \\times 0.5}{0.13 \\times 0.5 + 0.16 \\times 0.5 + 0.07 \\times 0.5 + 0.28 \\times 0.5 + 0.06 \\times 0.5}=0.08$\n",
    "<br>\n",
    "$P(C_2 | x_5)=\\frac{0.14 \\times 0.5}{0.07 \\times 0.5 + 0.07 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5 + 0.14 \\times 0.5}=0.25$\n",
    "\n",
    "<br>\n",
    "\n",
    "Therefore, the weights for $C_1$ are $w_1 = (0.18, 0.22, 0.1, 0.4, 0.08)$ and the weights for $C_2$ are $w_2 = (0.12, 0.12, 0.25, 0.25, 0.25)$\n",
    "\n",
    "<br>\n",
    "\n",
    "In the maximization step, mean and probability are re-estimated.\n",
    "<br>\n",
    "The mean is calculated as:\n",
    "$$\\mu_i = \\frac{\\sum^{n}_{j=1} w_{ij} \\cdot x_j}{\\sum^{n}_{j=1} w_{ij}}$$\n",
    "therefore, $\\mu_1 = 3.13$ and $\\mu_2 = 3.64$\n",
    "\n",
    "<br>\n",
    "\n",
    "The probability can be re-estimated as:\n",
    "$$P(C_i)=\\frac{\\sum^{n}_{j=1}w_{ij}}{n}$$\n",
    "therefore, $P(C_1)=0.196$ and $P(C_2)=0.198$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac2db607d06dbc",
   "metadata": {},
   "source": [
    "### 1.2 Chapter 14\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 1: Consider the 5-dimensional categorical data shown in Table 14.3 \n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\text{Point} & X_1 & X_2 & X_3 & X_4 & X_5\\\\\n",
    "\\hline\n",
    "x^{T}_{1} & 1 & 0 & 1 & 1 & 0\\\\\n",
    "x^{T}_{2} & 1 & 1 & 0 & 1 & 0\\\\\n",
    "x^{T}_{3} & 0 & 0 & 1 & 1 & 0\\\\\n",
    "x^{T}_{4} & 0 & 1 & 0 & 1 & 0\\\\\n",
    "x^{T}_{5} & 1 & 0 & 1 & 0 & 1 \\\\\n",
    "x^{T}_{6} & 0 & 1 & 1 & 0 & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "#### The similarity between categorical data points can be computed in terms of the number of matches and mismatches for the different attributes. Let $n_{11}$ be the number of attributes on which two points $x_i$ and $x_j$ assume the value 1, and let $n_{10}$ denote the number of attributes where $x_i$ takes value 1, but $x_j$ takes on the value of 0. Define $n_{01}$ and $n_{00}$ in a similar manner. The contingency table for measuring the similarity is then given as\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "& x_j & x_j & x_j \\\\\n",
    "\\hline\n",
    "x_i & & 1 & 0 \\\\\n",
    "\\hline\n",
    "x_i & 1 & n_{11} & n_{10} \\\\\n",
    "\\hline\n",
    "x_i & 0 & n_{01} & n_{00} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "#### Define the following similarity measures\n",
    "* #### Simple matching coefficient: $SMC(x_i, x_j)=\\frac{n_{11}+n_{00}}{n_{11}+n_{10}+n_{01}+n_{00}}$\n",
    "* #### Jaccard coefficient: $JC(x_i, x_j)=\\frac{n_{11}}{n_{11}+n_{10}+n_{01}}$\n",
    "* #### Rao's coefficient: $RC(x_i, x_j)=\\frac{n_{11}}{n_{11}+n_{10}+n_{01}+n_{00}}$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Find the cluster dendrograms produced by the hierarchical clustering algorithm under the following scenarios:\n",
    "<br>\n",
    "\n",
    "#### a.) We use single link with RC \n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & X_2 & X_3 & X_4 & X_5 & X_6 \\\\\n",
    "X_1 & 3/5 & 3/5 & 4/5 & 3/5 & 4/5\\\\\n",
    "X_2 & & 4/5 & 3/5 & 4/5 & 4/5\\\\\n",
    "X_3 & & & 4/5 & 4/5 & 4/5\\\\\n",
    "X_4 & & & & 5/5 & 4/5\\\\\n",
    "X_5 & & & & & 4/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& & X_3 & X_4 & X_5 & X_6\\\\\n",
    "X_1, X_2 & & 3/5 & 3/5 & 3/5 & 4/5\\\\\n",
    "X_3 & & & 4/5 & 4/5 & 4/5\\\\\n",
    "X_4 & & & & 5/5 & 4/5\\\\\n",
    "X_5 & & & & & 4/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & & & X_4 & X_5 & X_6\\\\\n",
    "X_1, X_2, X_3 & & & 3/5 & 3/5 & 4/5\\\\\n",
    "X_4 & & & & 5/5 & 4/5\\\\\n",
    "X_5 & & & & & 4/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "the dendrogram is created below:\n",
    "<br>\n",
    "```commandline\n",
    "1   2   3   4   5   6       Distance:\n",
    "|   |   |   |   |   |           \n",
    " 12     |   |   |   |           3/5\n",
    "  |     |   |   |   |\n",
    "    123     |   |   |           3/5\n",
    "     |      |   |   |\n",
    "       1234     |   |           3/5\n",
    "         |      |   |       \n",
    "           12345    |           3/5\n",
    "             |      |\n",
    "              123456            4/5\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### b.) We use complete link with SMC\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & X_2 & X_3 & X_4 & X_5 & X_6 \\\\\n",
    "X_1 & 2/5 & 1/5 & 3/5 & 2/5 & 3/5\\\\\n",
    "X_2 & & 3/5 & 1/5 & 4/5 & 3/5\\\\\n",
    "X_3 & & & 2/5 & 3/5 & 2/5\\\\\n",
    "X_4 & & & & 5/5 & 2/5\\\\\n",
    "X_5 & & & & & 3/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& & & X_2, X_4 & X_5 & X_6\\\\\n",
    "X_1, X_3 & & & 3/5 & 3/5 & 3/5\\\\\n",
    "X_2, X_4 & & & & 5/5 & 3/5\\\\\n",
    "X_5 & & & & & 3/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & & & & X_5 & X_6\\\\\n",
    "X_1, X_2, X_3, X_4 & & & & 5/5 & 3/5\\\\\n",
    "X_5 & & & & & 3/5\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "the dendrogram is created below:\n",
    "<br>\n",
    "```commandline\n",
    "1   3   2   4   6   5       Distance:\n",
    "|   |   |   |   |   |\n",
    "  13      24    |   |           1/5\n",
    "   |       |    |   |\n",
    "      1234      |   |           3/5\n",
    "        |       |   |\n",
    "          12346     |           3/5\n",
    "            |       |\n",
    "             123456             5/5\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### c.) We use group average with JC\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & X_2 & X_3 & X_4 & X_5 & X_6 \\\\\n",
    "X_1 & 2/4 & 1/3 & 3/4 & 2/4 & 3/4\\\\\n",
    "X_2 & & 3/4 & 1/3 & 4/5 & 3/4\\\\\n",
    "X_3 & & & 2/3 & 3/4 & 2/3\\\\\n",
    "X_4 & & & & 5/5 & 2/3\\\\\n",
    "X_5 & & & & & 3/4\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& & & X_2, X_4 & X_5 & X_6\\\\\n",
    "X_1, X_3 & & & 0.67 & 0.625 & 0.708\\\\\n",
    "X_2, X_4 & & & & 0.9 & 0.708\\\\\n",
    "X_5 & & & & & 0.6\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    " & & & & X_2, X_4 & X_5, X_6\\\\\n",
    "X_1, X_3 & & & & 0.625 & 0.666\\\\\n",
    "X_2, X_4 & & & & & 0.804\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "the dendrogram is created below:\n",
    "```commandline\n",
    "1   3   2   4   5   6       Distance:\n",
    "|   |   |   |   |   |\n",
    " 13       24    |   |           1/3\n",
    "  |        |    |   |\n",
    "  |        |      56            0.6\n",
    "  |        |       |\n",
    "     1234          |            0.625\n",
    "       |           |\n",
    "           123456\n",
    "```\n",
    "\n",
    "\n",
    "#### Exercise 3: Using the distance matrix from Table 14.4, use the average link method to generate hierarchical clusters. Show the merging distance thresholds. \n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "& A & B & C & D & E\\\\\n",
    "\\hline\n",
    "A & 0 & 1 & 3 & 2 & 4 \\\\\n",
    "\\hline\n",
    "B & & 0 & 3 & 2 & 3 \\\\\n",
    "\\hline\n",
    "C & & & 0 & 1 & 3 \\\\\n",
    "\\hline\n",
    "D & & & & 0 & 5 \\\\\n",
    "\\hline\n",
    "E & & & & & 0 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The minimum distance from the given dataset is 1 at $A,B$. Now we generate the distance matrix for the $AB$ cluster:\n",
    "<br>\n",
    "\n",
    "The distance from $AB$ to $C$,\n",
    "$$\n",
    "distance(AB, C)=average(distance(AB, C))=\\frac{distance(A, C) + distance(B, C)}{2}=\\frac{3+3}{2}=3\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The distance from $AB$ to $D$,\n",
    "$$\n",
    "distance(AB, D)=average(distance(AB, D))=\\frac{distance(A, D) + distance(B, D)}{2}=\\frac{2+2}{2}=2\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The distance from $AB$ to $E$,\n",
    "$$\n",
    "distance(AB, E)=average(distance(AB, E))=\\frac{distance(A, E) + distance(B, E)}{2}=\\frac{4+3}{2}=3.5\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "the new distance matrix with cluster $AB$:\n",
    "\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "& AB & C & D & E\\\\\n",
    "\\hline\n",
    "AB & 0 & 3 & 2 & 3.5\\\\\n",
    "\\hline\n",
    "C & & 0 & 1 & 3\\\\\n",
    "\\hline\n",
    "D & & & 0 & 5\\\\\n",
    "\\hline\n",
    "E & & & & 0\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Now we find the next minimum distance from table 2 to form the second cluster. The minimum distance from the table is 1 at $C,D$. Now we generate the distance matrix for the $CD$ cluster. \n",
    "\n",
    "<br>\n",
    "\n",
    "The distance from $CD$ to $AB$,\n",
    "$$\n",
    "distance(CD, AB)=average(distance(CD, AB))=\\frac{distance(AB, C) + distance(AB, D)}{2}=\\frac{3+2}{2}=2.5\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The distance from $CD$ to $E$,\n",
    "$$\n",
    "distance(CD, E)=average(distance(CD, E))=\\frac{distance(C, E) + distance(D, E)}{2}=\\frac{3+5}{2}=4\n",
    "$$\n",
    "\n",
    "the new distance matrix with cluster $AB$:\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "& AB & CD & E\\\\\n",
    "\\hline\n",
    "AB & 0 & 2.5 & 3.5\\\\\n",
    "\\hline\n",
    "CD &  & 0 & 4\\\\\n",
    "\\hline\n",
    "E & & & 0\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Now we find the next minimum distance from table 3 to form the third cluster. The minimum distance from the table is 2.5 at $AB,CD$. Now we generate the distance matrix for the $ABCD$ cluster. \n",
    "\n",
    "<br>\n",
    "\n",
    "The distance from $ABCD$ to $AB$,\n",
    "$$\n",
    "distance(ABCD, E)=average(distance(ABCD, E))=\\frac{distance(AB, E) + distance(CD, E)}{2}=\\frac{3.5+4}{2}=3.75\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "the new distance matrix with cluster $AB$:\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "& ABCD & E\\\\\n",
    "\\hline\n",
    "ABCD & 0 & 3.75\\\\\n",
    "\\hline\n",
    "E &  & 0\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The final merging distance from cluster $ABCD$ to $E$ is 3.75.\n",
    "<br>\n",
    "\n",
    "Final dendrogram:\n",
    "<br>\n",
    "```commandline\n",
    "A   B   C   D   E       Distance\n",
    "|   |   |   |   |           \n",
    " AB       CD    |           1\n",
    "  |        |    |\n",
    "     ABCD       |           2.5\n",
    "       |        |\n",
    "         ABCDE              3.75\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8fc4dfcc1124aa",
   "metadata": {},
   "source": [
    "### 1.3 Chapter 15\n",
    "\n",
    "<br> \n",
    "\n",
    "#### Exercise 1: Consider Figure 15.12 and answer the following questions, assuming that we use the Euclidian distance between points, and that $\\epsilon=2$ and $minpts=3$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### a.) List all the core points\n",
    "\n",
    "<br>\n",
    "\n",
    "p, q, v, r, s, t, w, k, h, d, a, b, c, e, i, f, g, j, n, o\n",
    "\n",
    "<br>\n",
    "\n",
    "#### b.) Is $a$ directly density reachable from $d$?\n",
    "\n",
    "<br>\n",
    "\n",
    "Yes, because $a$ and $d$ are core points and lie within the given radius $\\epsilon=2$\n",
    "\n",
    "<br>\n",
    "\n",
    "#### c.) Is $o$ density reachable from $i$? Show the intermediate points on the chain or the point where the chain breaks.\n",
    "\n",
    "<br>\n",
    "\n",
    "Yes, $o$ is density reachable from $i$. The intermediate points in the chain are: i, e, f, g, j, n, o\n",
    "\n",
    "<br>\n",
    "\n",
    "#### d.) Is density reachable a symmetric relationship, that is, if $x$ is reachable from $y$, does it imply that $y$ is density reachable from $x$? Why or why not?\n",
    "\n",
    "<br>\n",
    "\n",
    "No, density reachable is not a symmetric relationship. The reason for this is that if you have a core point x and y is density reachable from y through z, and if y is not a core point, then x is not density reachable from y. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### e.) Is $l$ density connected to $x$? Show the intermediate points that make them density connected or violate the property, respectively.\n",
    "\n",
    "<br>\n",
    "\n",
    "Yes, $l$ is density connected to $x$, the intermediate points that make them density connected is $w$, $t$. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### f.) Is density connected a symmetric relationship?\n",
    "\n",
    "<br>\n",
    "\n",
    "Yes. Density connected is a symmetric relationship.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### g.) Shoe the density-based clusters and the noise points. \n",
    "\n",
    "<br>\n",
    "\n",
    "There are two density based clusters.\n",
    "<br>\n",
    "Cluster 1: p, q, v, r, s, t, w, x, l, k, h, d, a\n",
    "<br>\n",
    "Cluster 2: m, i, e, b, c, f, g, j, n, o, u\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 2: Consider the points in Figure 15.13. Define the following distance measures;\n",
    "$$\n",
    "L_{\\infty}(x,y)=max^{d}_{i=1}{|x_i - y_i|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{1/2}(x,y)=(\\Sigma^{d}_{i=1} |x_i - y_i|^{1/2})^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{min}(x,y)=min^{d}_{i=1}{|x_i - y_i|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "L_{pow}(x,y)=(\\Sigma^{d}_{i=1} 2^{i-1} (x_i - y_i)^2)^{1/2}\n",
    "$$\n",
    "\n",
    "#### a.) Using $\\epsilon=2$, $minpts=5$, and $L_{infty}$ distance, find all core, border, and noise points. \n",
    "\n",
    "<br>\n",
    "\n",
    "Using $\\epsilon=2$ and $minpts=5$, the following points are core points: b, c, e, f, and g. These points all have at least 5 points within their $\\epsilon$-neighborhoods. The following points are border points: a, d, and h. These points are within the $\\epsilon$-neighborhoods of core points, but they do not have at least five points within their own $\\epsilon$-neighborhoods. The following points are noise points: i and j. These points are not within the $\\epsilon$-neighborhoods of any core points.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### b.) Show the shape of the ball radius $\\epsilon=4$ using the $L_{1/2}$ distance. Using $minpts=3$ show all the clusters found by DBSCAN.\n",
    "\n",
    "<br>\n",
    "\n",
    "The shape of the ball of radius $\\epsilon=4$ using the $L_{1/2}$ distance is a square with side length 8. This is because the $L_{1/2}$ distance is the same as the Euclidian distance when the dimensions of the data are equal. The points that fall within this square are a, b, c, e, f, g and d, h, i, j. Therefore, these points are all clustered together by DBSCAN. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### c.) Using $\\epsilon=1$, $minpts=6$, and $L_{min}$, list all core, border, and noise points.\n",
    "\n",
    "<br>\n",
    "\n",
    "Using $\\epsilon=1$ and $minpts=6$, the following points are core points: b, c, e, and f. These points all have at least 6 points within their $\\epsilon$-neighborhoods. The following points are border points: a and g. These points are within the $\\epsilon$-neighborhoods fo core points, but they do not have at least 6 points within their own $\\epsilon$-neighborhoods. The following points are noise points: d, h, i, and j. These points are not within the $\\epsilon$-neighborhoods of any core points. \n",
    "\n",
    "<br>\n",
    "\n",
    "#### d.) Using $\\epsilon=4$, $minpts=3$, and $L_{pow}$, show all clusters found by DBSCAN.\n",
    "\n",
    "<br>\n",
    "\n",
    "Using $\\epsilon=4$ and $minpts=3$, the following clusters are found by DBSCAN: a,b, c, e, f, g, and d, h, i, j. This is because all of the points in each cluster have at least 3 points within their $\\epsilon$-neighborhoods.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7621da4e1cd1655",
   "metadata": {},
   "source": [
    "### 1.4 Chapter 17\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 8: Show that the silhouette coefficients of a point lies in the interval [-1, +1].\n",
    "\n",
    "<br>\n",
    "\n",
    "The silhouette coefficient can be calculated by following these steps:\n",
    "* let $a$ be the average dissimilarity of the data point with all other points in the same cluster\n",
    "* let $b$ be the average dissimilarity of the data point with all points in the nearest neighboring cluster. \n",
    "* the silhouette coefficient for the data point is given by $\\frac{b-a}{max(a,b)}$\n",
    "<br>\n",
    "\n",
    "To prove that the silhouette coefficient lies in the interval [-1,+1], we need to show that $(b-a)$ is bounded by $-max(a,b)$ and $+max(a,b)$.\n",
    "\n",
    "* $max(a,b)$ is always greater than or equal to 0\n",
    "\n",
    "<br>\n",
    "\n",
    "$(b-a)$ has two cases:\n",
    "\n",
    "* Case 1: $b \\ge a$ \n",
    "  * In this case, $(b-a)$ is non-negative, or zero. The maximum value between $a$ and $b$ is $b$. Therefore, $\\frac{b-a}{max(a,b)}$ is between 0 and 1.\n",
    "* Case 2: $b < a$\n",
    "  * In this case, $(b-a)$ is negative. The maximum value between $a$ and $b$ is $a$. Therefore, $\\frac{b-a}{max(a,b)}$ is between -1 and 0.\n",
    "\n",
    "<br>\n",
    "\n",
    "From the above cases, we can observe that $\\frac{b-a}{max(a,b)}$ is always between -1 and +1.\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Exercise 10: Consider the dataset in Figure 17.10. Compute the silhouette coefficient for the point labeled $c$. Assume that the clusters are: $C_1={a,b,c,d,e}, C_2={g,i}, C_3={f,h,j}, C_4={k}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "silhouette coefficient = $\\frac{b-a}{max(a,b)}$\n",
    "\n",
    "distance between points = $\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "To find $a$:\n",
    "<br>\n",
    "$d(c,a)=2.9$\n",
    "<br>\n",
    "$d(e,d)=1.5$\n",
    "<br>\n",
    "$d(c,d)=2$\n",
    "<br>\n",
    "$d(c,e)=1$\n",
    "<br>\n",
    "\n",
    "$a=\\frac{2.9+1.5+2+1}{4}=1.87$\n",
    "\n",
    "<br>\n",
    "\n",
    "for cluster 3:\n",
    "<br>\n",
    "\n",
    "$d(c,f)=4.2$\n",
    "<br>\n",
    "$d(c,h)=5$\n",
    "<br>\n",
    "$d(c,l)=4.9$\n",
    "<br>\n",
    "$b=\\frac{4.2+5+4.9}{3}=4.6$\n",
    "<br>\n",
    "\n",
    "silhouette coefficient = $\\frac{4.6-1.87}{4.6}=0.59$\n",
    "\n",
    "<br>\n",
    "\n",
    "for cluster 2:\n",
    "<br>\n",
    "\n",
    "$d(c,g)=5.8$\n",
    "<br>\n",
    "$d(c,i)=5$\n",
    "<br>\n",
    "$b=\\frac{6+5}{2}=5.5$\n",
    "<br>\n",
    "\n",
    "silhouette coefficient = $\\frac{5.5-1.85}{5.5}=0.65$\n",
    "\n",
    "\n",
    "for cluster 4:\n",
    "<br>\n",
    "\n",
    "$d(c,k)=7$\n",
    "<br>\n",
    "\n",
    "silhouette coefficient = $\\frac{7-1.87}{7}=0.73$\n",
    "\n",
    "<br>\n",
    "\n",
    "According to the coefficients, cluster 4 is the farthest and cluster 3 is the nearest.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c434f7bfad66e9",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.geeksforgeeks.org/k-means-clustering-introduction/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://xlinux.nist.gov/dads/HTML/manhattanDistance.html#:~:text=Definition%3A%20The%20distance%20between%20two,y1%20%2D%20y2%7C.\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.cuemath.com/euclidean-distance-formula/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-expectation-maximization-algorithm/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://people.revoledu.com/kardi/tutorial/Similarity/SimpleMatching.html\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.statisticshowto.com/jaccard-index/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://medium.com/@Suraj_Yadav/silhouette-coefficient-explained-with-a-practical-example-assessing-cluster-fit-c0bb3fdef719#:~:text=The%20silhouette%20coefficient%20is%20a,clusters\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebfff736747f67e",
   "metadata": {},
   "source": [
    "## Practicum Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a7ff762e4c1d3",
   "metadata": {},
   "source": [
    "### 2.1 Problem 1\n",
    "\n",
    "#### Load the auto-mpg sample dataset from the UCI Machine Learning Repository into Python using a Pandas dataframe. Using only the continuous fields as features, impute any missing values with the mean, and perform a Hierarchical Clustering (Use sklearn.cluster.AgglomerativeClustering) with linkage set to average and the default affinity set to euclidean. Set the remaining parameters to obtain a shallow tree with 3 clusters as the target. Obtain the mean and variance values for each cluster, and compare these values to the values obtained for each class if we used origin as a class label. Is there a clear relationship between cluster assignment and class label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a3f5151b8055fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T17:56:26.065692Z",
     "start_time": "2024-07-09T17:56:20.915676Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster stats:\n",
      "                MPG            Displacement               Horsepower   \n",
      "              mean        var         mean          var        mean   \n",
      "Cluster                                                               \n",
      "0        26.177441  41.303375   144.304714  3511.485383   86.120275  \\\n",
      "1        14.528866   4.771033   348.020619  2089.499570  161.804124   \n",
      "2        43.700000   0.300000    91.750000    12.250000   49.000000   \n",
      "\n",
      "                          Weight                Acceleration            \n",
      "                var         mean            var         mean       var  \n",
      "Cluster                                                                 \n",
      "0        294.554450  2598.414141  299118.709664    16.425589  4.875221  \n",
      "1        674.075816  4143.969072  193847.051117    12.641237  3.189948  \n",
      "2          4.000000  2133.750000   21672.916667    22.875000  2.309167  \n",
      "\n",
      "Origin stats:\n",
      "               MPG            Displacement               Horsepower   \n",
      "             mean        var         mean          var        mean   \n",
      "Origin                                                               \n",
      "1       20.083534  40.997026   245.901606  9702.612255  119.048980  \\\n",
      "2       27.891429  45.211230   109.142857   509.950311   80.558824   \n",
      "3       30.450633  37.088685   102.708861   535.465433   79.835443   \n",
      "\n",
      "                          Weight                Acceleration            \n",
      "                var         mean            var         mean       var  \n",
      "Origin                                                                  \n",
      "1       1591.833657  3361.931727  631695.128385    15.033735  7.568615  \n",
      "2        406.339772  2423.300000  240142.328986    16.787143  9.276209  \n",
      "3        317.523856  2221.227848  102718.485881    16.172152  3.821779  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#load dataset\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model Year', 'Origin', 'Car Name']\n",
    "data = pd.read_csv('auto-mpg.data', delim_whitespace=True, names=column_names, na_values='?')\n",
    "\n",
    "#select only the continuous fields\n",
    "continuous_columns = ['MPG', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']\n",
    "data_continuous = data[continuous_columns]\n",
    "\n",
    "#impute missing values with the mean\n",
    "mean_imputer = SimpleImputer(strategy='mean')\n",
    "data_mean_imp = data_continuous.copy()\n",
    "data_mean_imp['Horsepower'] = mean_imputer.fit_transform(data_mean_imp[['Horsepower']])\n",
    "\n",
    "#scale the data\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_mean_imp)\n",
    "\n",
    "#perform hierarchical clustering with 3 clusters\n",
    "clustering = AgglomerativeClustering(n_clusters=3, linkage='average')\n",
    "clusters = clustering.fit_predict(data_scaled)\n",
    "\n",
    "#add cluster labels back to the dataframe\n",
    "data['Cluster'] = clusters\n",
    "\n",
    "#compute stats for each cluster\n",
    "cluster_stats = data.groupby('Cluster')[['MPG', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']].agg(['mean','var'])\n",
    "\n",
    "#compute stats for each origin class\n",
    "origin_stats = data.groupby('Origin')[['MPG', 'Displacement', 'Horsepower', 'Weight', 'Acceleration']].agg(['mean','var'])\n",
    "\n",
    "#print results\n",
    "print(\"Cluster stats:\\n\", cluster_stats)\n",
    "print()\n",
    "print(\"Origin stats:\\n\", origin_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d81ff66b8ab0a50",
   "metadata": {},
   "source": [
    "There seems to be a relationship between cluster assignments and origin class labels, especially for cluster 1 and possibly cluster 2. Cluster 1 aligns with origin 1 characteristics, where the cars typically have high displacement, high horsepower, heavier, and have lower fuel efficiency. Cluster 2 could possibly align with origin 3 in terms of high MPG and lower displacement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db3a1763bfcf692",
   "metadata": {},
   "source": [
    "### 2.2 Problem 2\n",
    "\n",
    "#### Load the Boston dataset (sklearn.datasets.load_boston()) into Python using a Pandas dataframe. Perform a K-means analysis on scaled data, with the number of clusters ranging from 2 to 6. Provide the Silhouette score to justify which value of k is optimal. Calculate the mean values for all features in each cluster for the optimal clustering - how do these values differ from the centroid coordinates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "724d2ad9f3ee54ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-14T03:11:34.647587Z",
     "start_time": "2024-07-14T03:11:34.539065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Scores:\n",
      "0.35010701730151716\n",
      "0.23388957997403387\n",
      "0.24394269176812938\n",
      "0.2455483784444133\n",
      "0.22579996220030626\n",
      "\n",
      "             CRIM         ZN      INDUS      CHAS       NOX        RM   \n",
      "Cluster                                                                 \n",
      "0        0.263946  17.477204   6.919818  0.069909  0.487215  6.456544  \\\n",
      "1        9.839575   0.000000  18.975085  0.067797  0.680124  5.965096   \n",
      "\n",
      "               AGE       DIS        RAD         TAX    PTRATIO           B   \n",
      "Cluster                                                                      \n",
      "0        56.382067  4.751124   4.474164  302.209726  17.818237  386.643891  \\\n",
      "1        91.238418  2.017920  18.983051  605.316384  19.640113  300.967345   \n",
      "\n",
      "             LSTAT       MEDV  \n",
      "Cluster                        \n",
      "0         9.417812  25.782067  \n",
      "1        18.666610  16.493220  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "#since load_boston() no longer works on scikit learn, I instead loaded the csv file of the boston housing prices\n",
    "#load dataset\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \n",
    "                'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = pd.read_csv('boston_house_prices.csv', header=None, names=column_names)\n",
    "\n",
    "#scaling the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "#perform k means analysis for cluster numbers from 2 to 6 and compute the silhouette score\n",
    "inertia = []\n",
    "cluster_range = range(2, 7)\n",
    "silhouette_scores = []\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    score = silhouette_score(scaled_data, kmeans.labels_)\n",
    "    silhouette_scores.append(score)\n",
    "\n",
    "print(\"Silhouette Scores:\")\n",
    "for i in range(0,5):\n",
    "    print(silhouette_scores[i])\n",
    "print()\n",
    "\n",
    "#perform k means clustering with the optimal number of clusters, k=2\n",
    "kmeans_optimal = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans_optimal.fit(scaled_data)\n",
    "\n",
    "#assign the cluster labels to the dataframe\n",
    "data['Cluster'] = kmeans_optimal.labels_\n",
    "\n",
    "#calculate the mean values for all features in each cluster\n",
    "cluster_means = data.groupby('Cluster').mean()\n",
    "\n",
    "print(cluster_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbdf4762e5cf003",
   "metadata": {},
   "source": [
    "Observing the silhouette scores, the most optimal number of clusters $k$ is 2, as it has the highest silhouette score of about 0.350. This suggests that clustering the dataset into two clusters results in the best separation and internal cohesion among the clusters compared to higher values of $k$.\n",
    "<br>\n",
    "These means values differ from the centroid coordinates. The centroids are calculated in the scaled space where each feature has been standardized (mean = 0, standard deviation = 1), thus the values of the centroids represent the position relative to the standard deviation of each feature across the entire dataset. The mean values are in the original scale of the data. The centroid values can be negative or positive reflecting their relative position from the mean (zero in scaled space). In contrast, the mean values reflect the actual average values of each feature within the clusters in the dataset's original units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99621bbe486d9a",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3 Problem 3\n",
    "\n",
    "#### Load the wine dataset (sklearn.datasets.load_wine()) into Python using a Pandas dataframe. Perform a K-Means analysis on scaled data, with the number of clusters set to 3. Given the actual class labels, calculate the Homogeneity/Completeness for the optimal k - what information do each of these metrics provide?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fda9013d12dc6a90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-09T19:38:28.648273Z",
     "start_time": "2024-07-09T19:38:28.632288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity score: 0.9126123393277744\n",
      "Completeness score: 0.9091008506516705\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "#load dataset\n",
    "wine = load_wine()\n",
    "dataframe = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "\n",
    "#add target column to the dataframe\n",
    "dataframe['target'] = wine.target\n",
    "\n",
    "#scaling the features\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(dataframe)\n",
    "\n",
    "#initialize K-Means with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "#adding a cluster column to the dataframe to see what cluster a row is in\n",
    "dataframe['cluster'] = kmeans.labels_\n",
    "\n",
    "#assuming target from the dataset is the true label\n",
    "true_labels = wine.target\n",
    "\n",
    "#calculating homogeneity score and completeness score\n",
    "homogeneity_score = metrics.homogeneity_score(true_labels, kmeans.labels_)\n",
    "completeness_score = metrics.completeness_score(true_labels, kmeans.labels_)\n",
    "\n",
    "print(\"Homogeneity score:\", homogeneity_score)\n",
    "print(\"Completeness score:\", completeness_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fef61e7e2c3e5a",
   "metadata": {},
   "source": [
    "A homogeneity score that is close to 1 means that the clusters are highly homogeneous. This means that most of the clusters consist almost of data points belonging to a single class. A completeness score close to 1 means that nearly all data points that are members of a specific class are grouped together into the same cluster. This indicates that the clustering model has successfully manged to include most members of each class within a single cluster. In our case, we have a high value for homogeneity and completeness. This means our clusters are pure and complete, there are no trade-offs where one metric is sacrificed to improve the other. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3230e82f9120e268",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering\n",
    "\n",
    "<br>\n",
    "\n",
    "https://github.com/scikit-learn/scikit-learn/blob/main/sklearn/datasets/data/boston_house_prices.csv\n",
    "\n",
    "<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "<br>\n",
    "\n",
    "https://www.geeksforgeeks.org/ml-handle-missing-data-with-simple-imputer/\n",
    "\n",
    "<br>\n",
    "\n",
    "https://stackoverflow.com/questions/51138686/how-to-use-silhouette-score-in-k-means-clustering-from-sklearn-library\n",
    "\n",
    "<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.homogeneity_score.html\n",
    "\n",
    "<br>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.completeness_score.html\n",
    "\n",
    "<br>\n",
    "\n",
    "ChatGPT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
